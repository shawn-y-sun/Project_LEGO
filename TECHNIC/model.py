# =============================================================================
# module: model.py
# Purpose: Define base and OLS regression models with testing and reporting hooks
# Dependencies: pandas, numpy, statsmodels, typing, .test.TestSet, .report.OLS_ModelReport
# =============================================================================

from abc import ABC, abstractmethod
import pandas as pd
import numpy as np
import statsmodels.api as sm
from typing import Optional, Any, Callable, Type, Dict
from statsmodels.stats.outliers_influence import variance_inflation_factor
from .test import *
from .report import ModelReportBase, OLS_ModelReport

class ModelBase(ABC):
    """
    Abstract base class for statistical models with testing and reporting.

    Parameters
    ----------
    X : DataFrame
        In-sample features.
    y : Series
        In-sample target.
    X_out : DataFrame, optional
        Out-of-sample features.
    y_out : Series, optional
        Out-of-sample target.
    spec_map : dict, optional
        Variable grouping for tests (e.g., 'common', 'group').
    testset_func : callable, optional
        Builds initial mapping of tests.
    test_update_func : callable, optional
        Updates or adds tests post initial mapping.
    testset_cls : type, default TestSet
        Class for aggregating ModelTestBase instances into a TestSet.
    report_cls : type, optional
        Class for generating model reports.
    """
    def __init__(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        X_out: Optional[pd.DataFrame] = None,
        y_out: Optional[pd.Series] = None,
        spec_map: Optional[Dict[str, Any]] = None,
        testset_func: Optional[Callable[['ModelBase'], Dict[str, ModelTestBase]]] = None,
        test_update_func: Optional[Callable[['ModelBase'], Dict[str, Any]]] = None,
        testset_cls: Type = TestSet,
        report_cls: Optional[Type] = None
    ):
        # In-sample and out-of-sample data
        self.X = X
        self.y = y
        self.X_out = X_out if X_out is not None else pd.DataFrame()
        self.y_out = y_out if y_out is not None else pd.Series(dtype=float)
        # Specification grouping for tests
        self.spec_map = spec_map or {}
        # Test configuration
        self.testset_func = testset_func
        self.test_update_func = test_update_func
        self.testset_cls = testset_cls
        self.testset: Optional[TestSet] = None
        # Reporting configuration
        self.report_cls = report_cls
        # Model metadata
        self.coefs_ = None
        self.is_fitted = False
        # Cache for out-of-sample predictions
        self._y_pred_out: Optional[pd.Series] = None

    @abstractmethod
    def fit(self) -> 'ModelBase':
        """
        Fit the model to in-sample data.

        Returns
        -------
        self : ModelBase
        """
        ...

    @abstractmethod
    def predict(self, X_new: pd.DataFrame) -> pd.Series:
        """
        Generate predictions for new data.
        """
        ...

    @property
    def y_pred_out(self) -> pd.Series:
        """
        Out-of-sample predictions generated by calling predict on X_out.

        Returns empty Series if X_out is empty.
        """
        if self.X_out.empty:
            return pd.Series(dtype=float)
        self._y_pred_out = self.predict(self.X_out)
        return self._y_pred_out

    @property
    def report(self) -> ModelReportBase:
        """
        Build and return the report instance using report_cls and this model.
        """
        if not self.report_cls:
            raise ValueError("No report_cls provided for building report.")
        # Now report_cls must accept only model=self
        return self.report_cls(model=self)


    def load_testset(
        self,
        testset_func: Optional[Callable[['ModelBase'], Dict[str, ModelTestBase]]] = None,
        test_update_func: Optional[Callable[['ModelBase'], Dict[str, Any]]] = None
    ) -> TestSet:
        """
        Rebuild TestSet from provided functions and cache it.
        """
        func_init = testset_func or self.testset_func
        if func_init is None:
            raise ValueError("No testset_func provided.")
        tests = func_init(self)
        func_update = test_update_func or self.test_update_func
        if func_update:
            updates = func_update(self)
            for alias, val in updates.items():
                if isinstance(val, ModelTestBase):
                    tests[alias] = val
                elif isinstance(val, dict):
                    if alias in tests:
                        for k, v in val.items(): setattr(tests[alias], k, v)
                    else:
                        raise KeyError(f"Unknown test '{alias}' in update_map")
                else:
                    raise TypeError("test_update_map values must be ModelTestBase or kwargs dict")
        # Apply aliases and assemble TestSet
        for alias, obj in tests.items(): obj.alias = alias
        self.testset = self.testset_cls(tests)
        return self.testset


# Default PPNR OLS testset builder
def ppnr_ols_testset_func(mdl: 'ModelBase') -> Dict[str, ModelTestBase]:
    """
    Pre-defined TestSet for PPNR OLS models with improved group labels:
    - In-sample R²
    - Individual significance (common drivers)
    - Joint F-tests (group drivers)
    - Residual stationarity & normality
    """
    tests: Dict[str, ModelTestBase] = {}

    # In-sample R²
    tests['In-Sample R²'] = R2Test(
        r2=mdl.rsquared,
        alias='In-Sample R²',
        filter_mode='strict'
    )

    # Common-driver significance
    common = mdl.spec_map.get('common', [])
    if common:
        tests['Common Driver Significance'] = SignificanceTest(
            pvalues=mdl.pvalues.loc[common],
            alias='Common Driver Significance',
            filter_mode='strict'
        )

    # Group-driver significance with '^' labels
    for grp in mdl.spec_map.get('group', []):
        # list of names
        if isinstance(grp, (list, tuple)):
            names = list(grp)
            parts = [name.split(':', 1) if ':' in name else [None, name] for name in names]
            prefixes = [p[0] for p in parts]
            suffixes = [p[1] for p in parts]
            # detect common prefix
            if None not in prefixes and len(set(prefixes)) == 1:
                prefix = prefixes[0] + ':'
                label_body = '^'.join(suffixes)
                group_label = f"{prefix}{label_body}"
            else:
                group_label = '^'.join(names)
            vars_for = names
        else:
            group_label = str(grp)
            vars_for = [grp]

        alias = f"Group Driver F-Test {group_label}"
        tests[alias] = FTest(
            model_result=mdl.fitted,
            vars=vars_for,
            alias=alias,
            filter_mode='strict'
        )

    # Residual diagnostics
    tests['Residual Stationarity'] = StationarityTest(
        series=mdl.resid,
        alias='Residual Stationarity',
        filter_mode='strict'
    )
    tests['Residual Normality'] = NormalityTest(
        series=mdl.resid,
        alias='Residual Normality',
        filter_mode='moderate'
    )

    return tests


class OLS(ModelBase):
    """
    Ordinary Least Squares regression model with built-in testing and reporting.
    """
    def __init__(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        X_out: Optional[pd.DataFrame] = None,
        y_out: Optional[pd.Series] = None,
        spec_map: Optional[Dict[str, Any]] = None,
        testset_func: Callable[['ModelBase'], Dict[str, ModelTestBase]] = ppnr_ols_testset_func,
        test_update_func: Optional[Callable[['ModelBase'], Dict[str, Any]]] = None,
        report_cls: Type = OLS_ModelReport
    ):
        super().__init__(
            X=X,
            y=y,
            X_out=X_out,
            y_out=y_out,
            spec_map=spec_map,
            testset_func=testset_func,
            test_update_func=test_update_func,
            testset_cls=TestSet,
            report_cls=report_cls
        )
        # Fit result placeholders
        self.fitted = None
        self.params = None
        self.pvalues = None
        self.rsquared = None
        self.rsquared_adj = None
        self.y_fitted_in = None
        self.resid = None
        self.bse = None
        self.vif = None

    def fit(self) -> 'OLS':
        """
        Fit OLS and store coefficients, residuals, VIF, etc.
        """
        Xc = sm.add_constant(self.X)
        res = sm.OLS(self.y, Xc).fit()
        self.fitted = res
        self.params = res.params
        self.pvalues = res.pvalues
        self.rsquared = res.rsquared
        self.rsquared_adj = res.rsquared_adj
        self.y_fitted_in = res.fittedvalues
        self.resid = res.resid
        self.bse = res.bse
        # Compute VIF on X with intercept
        self.vif = pd.Series({
            col: variance_inflation_factor(Xc.values, i)
            for i, col in enumerate(Xc.columns)
        })
        self.coefs_ = self.params
        self.is_fitted = True
        # Automatically load testset after fitting
        self.load_testset()
        return self

    def predict(self, X_new: pd.DataFrame) -> pd.Series:
        """
        Predict using the fitted statsmodels results.
        """
        if not self.is_fitted or self.fitted is None:
            raise RuntimeError("Model has not been fitted yet.")
        Xc_new = sm.add_constant(X_new, has_constant='add')
        return self.fitted.predict(Xc_new)
    
    @property
    def param_measures(self) -> Dict[str, Dict[str, Any]]:
        """
        Parameter measures: coefficient, pvalue, VIF, and standard error for each term.
        """
        return {
            var: {
                'coef': float(self.params[var]),
                'pvalue': float(self.pvalues[var]),
                'vif': float(self.vif.get(var, np.nan)),
                'std': float(self.bse.get(var, np.nan))
            }
            for var in self.params.index
        }

    @property
    def in_perf_measures(self) -> Dict[str, Any]:
        """
        In-sample performance: R², adj-R², ME, MAE, RMSE.
        """
        errors = self.y - self.y_fitted_in
        return {
            'r2': float(self.rsquared),
            'adj_r2': float(self.rsquared_adj),
            'me': float(np.max(np.abs(errors))),
            'mae': float(np.mean(np.abs(errors))),
            'rmse': float(np.sqrt((errors ** 2).mean()))
        }

    @property
    def out_perf_measures(self) -> Dict[str, Any]:
        """
        Out-of-sample performance using y_pred_out.
        """
        if self.X_out.empty or self.y_out.empty:
            return {}
        errors = self.y_out - self.y_pred_out
        return {
            'me': float(np.max(np.abs(errors))),
            'mae': float(np.mean(np.abs(errors))),
            'rmse': float(np.sqrt((errors ** 2).mean()))
        }
